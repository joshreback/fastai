{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d942f5d3",
   "metadata": {},
   "source": [
    "Goal is to complete the \"Further Research\" project: \n",
    "\n",
    "> Complete all the steps in this chapter using the full MNIST datasets (that is, for all digits, not just 3s and 7s). This is a significant project and will take you quite a bit of time to complete! You'll need to do some of your own research to figure out how to overcome some obstacles you'll meet on the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1214797c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! [ -e /content ] && pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()\n",
    "from fastai.vision.all import *\n",
    "from fastbook import *\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673b27d5",
   "metadata": {},
   "source": [
    "First change: need to use the full MNIST dataset. Not just the sample that only has 3's and 7's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "55104049",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.MNIST)\n",
    "Path.BASE_PATH = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0b3e6aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('testing'),Path('training')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "69ec5a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#10) [Path('training/9'),Path('training/3'),Path('training/6'),Path('training/0'),Path('training/7'),Path('training/2'),Path('training/4'),Path('training/8'),Path('training/5'),Path('training/1')]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'training').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2488b841",
   "metadata": {},
   "source": [
    "As expected, a directory for every digit.\n",
    "\n",
    "Let's confirm it's the same number of pixels as from the sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c1cabe3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+tjwr4fl8U+KNP0SGQxNdyhDKE3eWvVmxkZwATjIrMuYfs91NBu3eW7JnGM4OKior074It9j8Q63q+0MNP0eebb0Yn5eh7Hrz715k7s7s7sWZjkk9SaSivTvAURt/hV491C3tlluzBFbFiwGyJyd55Ppz+ArzGiiiiiv/2Q==\n",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAjklEQVR4AWNgGHBg898PyQ1MSGwg8/8/ZD6aZA2yHAOqpLQiHslIVRRJVM6Rv+e5kERQjWVg+PUNjySSFAOqg0QFGGagyCJzPP6+M0Xmo9uJLIdqLAMDMwuyLJrOi8dxSoYjywDZyDqNfRjeo0kjuP5//zoieGg6GRhu3sIj+fIlHkm7JjySDL9QJAeCAwDxLRe5Av1bugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im1_path = (path/'training/1').ls().sorted()[0]\n",
    "im1 = Image.open(im1_path)\n",
    "im1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "45053df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 28])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor(im1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1838015c",
   "metadata": {},
   "source": [
    "Good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b9fdd",
   "metadata": {},
   "source": [
    "### Approach for handling categorical values\n",
    "\n",
    "The first big question to answer is how we're going to encode the labels. For 3 vs. 7, it is straightforward: 1 is 3, 0 is 7. \n",
    "\n",
    "For classifying each of the 10 digits, it's a bit more complex. \n",
    "\n",
    "The naive way would be to have the model output a number. The loss would be the distance from the prediction to the label. And the accuracy would be measured as the % of times the model's prediction was closest to the label. \n",
    "\n",
    "The issue with this approach is that it treats the labels as continuous values, when they are categorical/discrete values. For example, any given 1 and 9 are more similar to each other than any given 1 and 8. \n",
    "\n",
    "However, this method of measuring loss would treat a 9 prediction as further from the 1 label as an 8 prediction, which doesn't seem correct. Also, relatedly, it doesn't allow the model to be able to express a confidence in a particular number, it is forced to make one scalar prediction which encodes false assumptions about digit similarity (as described above).\n",
    "\n",
    "Instead, what we must do is treat each label as discrete and unrelated. So a \"1\" prediction isn't inherently closer to a \"2\". The closeness must depend on the model's prediction for a particular example, and not based on how we construct the space of labels. \n",
    "\n",
    "The correct way to handle categorical labels is to use 'one-hot encoding', where the model outputs a prediction quantity for each of the 10 possible digits. Each digit label is encoded as nine 0's with one 1 corresponding to the actual label. Then, we measure accuracy as the number of times the model's most confident prediction is the correct label, and we encode loss as the distance between the model's prediction for the correct label. The prediction should correspond to the _probability_ or _confidence_ that that particular digit is what's being represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "778cb20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_labels = F.one_hot(torch.arange(0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f949c4a",
   "metadata": {},
   "source": [
    "We need to arrange this so that:\n",
    "\n",
    "the first row: `[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]` is the label for 0\n",
    "\n",
    "the second row: `[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]` is the label for 1\n",
    "\n",
    "etc\n",
    "\n",
    "In the MNIST_SAMPLE, we had a line like this: \n",
    "\n",
    "`train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)`\n",
    "\n",
    "We'll try something where instead of `[1]` meaning 3, we'll instead of `[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]` means 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b03fe4",
   "metadata": {},
   "source": [
    "We also need to set up `train_x` so it includes data for each possible digit, rather than having the code for 3 and 7 be copied. Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6d5572e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training 0\n",
      "training 1\n",
      "training 2\n",
      "training 3\n",
      "training 4\n",
      "training 5\n",
      "training 6\n",
      "training 7\n",
      "training 8\n",
      "training 9\n",
      "testing 0\n",
      "testing 1\n",
      "testing 2\n",
      "testing 3\n",
      "testing 4\n",
      "testing 5\n",
      "testing 6\n",
      "testing 7\n",
      "testing 8\n",
      "testing 9\n"
     ]
    }
   ],
   "source": [
    "# train_x should ultimately have a similar shape as in mnist_sample:\n",
    "# (<num_images>, 784)\n",
    "# train_y should have a slightly different shape:\n",
    "# - each element should be a list (rather than a one element tensor)\n",
    "def compute_dset(mode='training'):\n",
    "    allowed_modes = {'training', 'testing'} \n",
    "    if mode not in allowed_modes:\n",
    "        raise Exception(f'path must be in {allowed_modes}')\n",
    "\n",
    "    x = []\n",
    "    stacked_tensors = []\n",
    "    y = []\n",
    "    for digit in range(0, 10):\n",
    "        print(mode, digit)\n",
    "        digit_paths = (path/f'{mode}/{digit}').ls().sorted()\n",
    "        digit_tensors = [tensor(Image.open(o)) for o in digit_paths]\n",
    "        stacked_digit = torch.stack(digit_tensors).float()/255\n",
    "        stacked_tensors.append(stacked_digit)\n",
    "        y += [possible_labels[digit]]*len(digit_paths)\n",
    "    x = torch.cat(stacked_tensors).view(-1, 28*28)\n",
    "    dset = list(zip(x, y))\n",
    "    return dset\n",
    "\n",
    "batch_size = 256\n",
    "train_dset = compute_dset(mode='training')\n",
    "train_dl = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "valid_dset = compute_dset(mode='testing')\n",
    "valid_dl = DataLoader(valid_dset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "46943cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = train_dset[0]\n",
    "x, y\n",
    "x.shape, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b387fe9",
   "metadata": {},
   "source": [
    "This is inline with what we want (I think). \n",
    "\n",
    "Input is 784 pixels, output is a list of 10 possible values, one-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4bd4ee",
   "metadata": {},
   "source": [
    "Now, let's construct the model. \n",
    "\n",
    "The model must output 10 predictions. Each of those 10 predictions should align with the likelihood of that particular digit. \n",
    "\n",
    "Below:\n",
    "- n_out1 is the output from the first linear layer. This is arbitrary and I'll tweak it to get better model performance. \n",
    "- nn.Linear must output 10 predictions, each corresponding to a digit. \n",
    "- I then need to run these through the `softmax` function so it converts the predictions to what they need to be: relative probabilities that sum to 1 (as probabilities should)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "de98fb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out1 = 30 \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(28*28,n_out1),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(n_out1,10)  # 10 output channels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1859aa",
   "metadata": {},
   "source": [
    "Let's see how to get a singular prediction out of the model to verify: \n",
    "- what a set of predictions looks like\n",
    "- how to softmax those predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f3400b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(x.view(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "aaefc5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0299, -0.0734,  0.1766, -0.0362, -0.0795,  0.0382,  0.0314, -0.0877, -0.0388, -0.0339], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(x)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f186e10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2746/4116768088.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  softmaxxed = F.softmax(test.float())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[9.9954e-01, 1.2335e-04, 3.3531e-04],\n",
       "        [9.0031e-02, 2.4473e-01, 6.6524e-01],\n",
       "        [9.0031e-02, 2.4473e-01, 6.6524e-01]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tensor([11, 2, 3], [4, 5, 6], [7, 8, 9])\n",
    "softmaxxed = F.softmax(test.float())\n",
    "softmaxxed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ea3ca3",
   "metadata": {},
   "source": [
    "So far so good. \n",
    "\n",
    "Now, let's ensure we can take a softmax on a tensor of predictions, since our `mnist_loss` is going to feed a minibatch's worth of predictions to the loss function, and this is where we must softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54b2f1a",
   "metadata": {},
   "source": [
    "This clarifies the first part of the mnist_loss function - how to take the softmax. \n",
    "\n",
    "Now that the predictions are appropriately softmaxxed - how do we compute a suitable loss?\n",
    "\n",
    "One idea is to just take the mean difference between the correct label (i.e, the singular `1` in the array), and the corresponding probability of that digit for row of the minibatch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2947338f",
   "metadata": {},
   "source": [
    "To do that, I need to be able to access the `index` of the 1 in the target, and then use that index to access the prediction quantity in the prediction tensor, and then take the mean of 1 minus that value. Let's do this using `argmax` and let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "53f94a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 2])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "02966dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets):\n",
    "    predictions = F.softmax(predictions)\n",
    "    index_of_1 = torch.argmax(targets, axis=1)\n",
    "    loss = (1 - predictions[torch.arange(len(predictions)), index_of_1])\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebc7d1d",
   "metadata": {},
   "source": [
    "Similar task to compute accuracy: compare whether argmax of predictions equals argmax of actuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "decf3b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(model_preds, yb):\n",
    "    most_likely_pred = torch.argmax(model_preds, axis=1)\n",
    "    correct_ans = torch.argmax(yb, axis=1)\n",
    "    return (most_likely_pred == correct_ans).sum() / len(model_preds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2738a8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders(train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "095ddc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.744367</td>\n",
       "      <td>0.670682</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.429554</td>\n",
       "      <td>0.393185</td>\n",
       "      <td>0.656300</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.341814</td>\n",
       "      <td>0.313278</td>\n",
       "      <td>0.741100</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.244871</td>\n",
       "      <td>0.222805</td>\n",
       "      <td>0.830200</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.183521</td>\n",
       "      <td>0.163355</td>\n",
       "      <td>0.887200</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.152111</td>\n",
       "      <td>0.140334</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.136489</td>\n",
       "      <td>0.128365</td>\n",
       "      <td>0.899400</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.129734</td>\n",
       "      <td>0.120194</td>\n",
       "      <td>0.904300</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.120931</td>\n",
       "      <td>0.113949</td>\n",
       "      <td>0.908100</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.114008</td>\n",
       "      <td>0.109864</td>\n",
       "      <td>0.910400</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.111475</td>\n",
       "      <td>0.106240</td>\n",
       "      <td>0.912800</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.108266</td>\n",
       "      <td>0.103035</td>\n",
       "      <td>0.914100</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.103504</td>\n",
       "      <td>0.101197</td>\n",
       "      <td>0.914300</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.103707</td>\n",
       "      <td>0.099696</td>\n",
       "      <td>0.916800</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.100910</td>\n",
       "      <td>0.096502</td>\n",
       "      <td>0.919200</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.100899</td>\n",
       "      <td>0.095736</td>\n",
       "      <td>0.917700</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.096855</td>\n",
       "      <td>0.093519</td>\n",
       "      <td>0.920800</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.096911</td>\n",
       "      <td>0.092748</td>\n",
       "      <td>0.920500</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.094706</td>\n",
       "      <td>0.091156</td>\n",
       "      <td>0.921900</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.094809</td>\n",
       "      <td>0.090149</td>\n",
       "      <td>0.922900</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.092189</td>\n",
       "      <td>0.089022</td>\n",
       "      <td>0.921800</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.091345</td>\n",
       "      <td>0.088314</td>\n",
       "      <td>0.923000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.087237</td>\n",
       "      <td>0.087210</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.087058</td>\n",
       "      <td>0.086695</td>\n",
       "      <td>0.923500</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.086575</td>\n",
       "      <td>0.085163</td>\n",
       "      <td>0.925600</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.085547</td>\n",
       "      <td>0.085110</td>\n",
       "      <td>0.926100</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.083374</td>\n",
       "      <td>0.084173</td>\n",
       "      <td>0.926200</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.085167</td>\n",
       "      <td>0.083764</td>\n",
       "      <td>0.926400</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.084315</td>\n",
       "      <td>0.083145</td>\n",
       "      <td>0.926700</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.082935</td>\n",
       "      <td>0.082389</td>\n",
       "      <td>0.927700</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.081674</td>\n",
       "      <td>0.081630</td>\n",
       "      <td>0.928800</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.081788</td>\n",
       "      <td>0.081212</td>\n",
       "      <td>0.929100</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.080716</td>\n",
       "      <td>0.080833</td>\n",
       "      <td>0.928700</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.080726</td>\n",
       "      <td>0.079934</td>\n",
       "      <td>0.930600</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.080333</td>\n",
       "      <td>0.079556</td>\n",
       "      <td>0.930200</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.080246</td>\n",
       "      <td>0.078968</td>\n",
       "      <td>0.930800</td>\n",
       "      <td>00:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.078787</td>\n",
       "      <td>0.078650</td>\n",
       "      <td>0.930800</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.079023</td>\n",
       "      <td>0.078374</td>\n",
       "      <td>0.930600</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.075867</td>\n",
       "      <td>0.078047</td>\n",
       "      <td>0.930100</td>\n",
       "      <td>00:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.075371</td>\n",
       "      <td>0.077406</td>\n",
       "      <td>0.931400</td>\n",
       "      <td>00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2746/575856330.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  predictions = F.softmax(predictions)\n"
     ]
    }
   ],
   "source": [
    "learn = Learner(dls, model, opt_func=SGD,\n",
    "                loss_func=mnist_loss, metrics=batch_accuracy)\n",
    "lr = 0.1\n",
    "learn.fit(40, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db6169b",
   "metadata": {},
   "source": [
    "So, high level, this works! Gets to around 93% accuracy a nd tops out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb24e416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
